# NOTE
# This docker-compose override is slightly different than most of the other chapters. The main difference is that
# we use a custom Dockerfile and image for a number of components, which required the edge3 provider to be available
# at startup. Installing the edge3 provider package is done in the Dockerfile.edge file. Unfortunately, installing
# via the usual approach of adding it to the _PIP_ADDITIONAL_REQUIREMENTS_ environment variable does not work as
# the package is then either installed after the work has been done (in the case of Airflow init) or is not available
# yet when the Airflow component starts. This latter one could be circumvented by overriding the entrypoint, but
# for the sake of consistency, we opted to use the same custom Dockerfile for all affected components.

# This environment contains both a Celery Executor worker and an Edge Executor worker. This is to demonstrate
# the Edge Executor in action (although it being in the same environment as the other Airflow components kind of
# defeats the purpose ;-).

x-airflow-common:
  &airflow-common
  environment:
    AIRFLOW__CORE__EXECUTOR: 'CeleryExecutor,airflow.providers.edge3.executors.EdgeExecutor'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__TEST_CONNECTION: 'Enabled'
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL : 60
    AIRFLOW__METRICS__STATSD_ON: 'true'
    AIRFLOW__METRICS__STATSD_HOST: 'statsd_exporter'
    AIRFLOW__METRICS__STATSD_PORT: 9125
    AIRFLOW__METRICS__STATSD_PREFIX: 'airflow'
    AIRFLOW__API_AUTH__JWT_SECRET: foo
    AIRFLOW__WEBSERVER__SECRET_KEY: bar
    AIRFLOW__EDGE__API_URL: 'http://airflow-apiserver:8080/edge_worker/v1/rpcapi'
    AIRFLOW__EDGE__API_ENABLED: "true"
    AIRFLOW__CORE__INTERNAL_API_SECRET_KEY: foo

    # Dag specific connection setup
    AIRFLOW_CONN_MOVIELENS: http://airflow:airflow@movielens
    MOVIELENS_USER: airflow
    MOVIELENS_PASSWORD: airflow
  volumes:
    - airflow-data-volume:/data

services:
  airflow-apiserver:
    build:
      context: .
      dockerfile: Dockerfile.edge
    <<: *airflow-common

  airflow-scheduler:
    <<: *airflow-common

  airflow-worker:
    <<: *airflow-common

  airflow-triggerer:
    <<: *airflow-common

  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.edge
    <<: *airflow-common

  airflow-cli:
    <<: *airflow-common

  airflow-dag-processor:
    <<: *airflow-common

  airflow-edge-worker:
    build:
      context: .
      dockerfile: Dockerfile.edge
    <<: *airflow-common
    command: edge worker
    restart: always
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags  # needed for the Edge executor to be able to access the dags
    depends_on:
      airflow-apiserver:
        condition: service_healthy

  statsd_exporter:
    image: prom/statsd-exporter:v0.28.0
    restart: always
    volumes:
      - ./files/statsd_mapping.yml:/tmp/statsd_mapping.yml
    ports:
      - "9102:9102"
      - "9125:9125/udp"
    command: --statsd.mapping-config=/tmp/statsd_mapping.yml

  prometheus:
    image: prom/prometheus:v3.1.0
    restart: always
    volumes:
      - ./files/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    command:
      - --web.enable-admin-api
      - --web.enable-lifecycle
      # Flags below are defaults, but must be added explicitly, otherwise would be overridden by flags above
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --web.console.libraries=/usr/share/prometheus/console_libraries
      - --web.console.templates=/usr/share/prometheus/consoles

  grafana:
    image: grafana/grafana:11.4.0
    restart: always
    ports:
      - "3000:3000"

  redis_exporter:
    image: oliver006/redis_exporter:v1.67.0-alpine
    ports:
      - "9121:9121"
    command: --redis.addr=redis://redis:6379

  airflow-data:
    build:
      context: .
      dockerfile: Dockerfile.edge
    <<: *airflow-common
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
        fi
        chown -R "${AIRFLOW_UID}:0" /data
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    user: "0:0"

volumes:
  airflow-data-volume:
