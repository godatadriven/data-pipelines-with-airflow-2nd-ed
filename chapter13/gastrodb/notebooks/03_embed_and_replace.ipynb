{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Mapping, Sequence, cast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet('/home/isma/repos/book/data-pipelines-with-airflow-2nd-ed/chapter13_genai/recipe_book/notebooks/splitted_with_vectors.parquet')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_column = \"chunk_sha\"\n",
    "\n",
    "unique_columns = sorted(data.columns.to_list())\n",
    "\n",
    "data = data.drop_duplicates(subset=[uuid_column], keep=\"first\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_document_to_uuid_map(\n",
    "    data: Sequence[Mapping], group_key: str, get_value\n",
    ") -> dict[str, set]:\n",
    "    \"\"\"Prepare the map of grouped_key to set.\"\"\"\n",
    "    grouped_key_to_set: dict = {}\n",
    "    for item in data:\n",
    "        document_url = item[group_key]\n",
    "\n",
    "        if document_url not in grouped_key_to_set:\n",
    "            grouped_key_to_set[document_url] = set()\n",
    "\n",
    "        grouped_key_to_set[document_url].add(get_value(item))\n",
    "    return grouped_key_to_set\n",
    "\n",
    "\n",
    "input_documents_to_uuid = _prepare_document_to_uuid_map(\n",
    "    data=data.to_dict(\"records\"),\n",
    "    group_key=\"chunk\",\n",
    "    get_value=lambda x: x[\"chunk_sha\"],\n",
    ")\n",
    "\n",
    "input_documents_to_uuid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_to_uuid: dict = {}\n",
    "document_keys = set(data[document_column])\n",
    "while True:\n",
    "    collection = self.get_collection(collection_name)\n",
    "    data_objects = collection.query.fetch_objects(\n",
    "        filters=Filter.any_of(\n",
    "            [Filter.by_property(document_column).equal(key) for key in document_keys]\n",
    "        ),\n",
    "        return_properties=[document_column],\n",
    "        limit=limit,\n",
    "        offset=offset,\n",
    "    )\n",
    "    if len(data_objects.objects) == 0:\n",
    "        break\n",
    "    offset = offset + limit\n",
    "\n",
    "    if uuid_column in data_objects.objects[0].properties:\n",
    "        data_object_properties = [obj.properties for obj in data_objects.objects]\n",
    "    else:\n",
    "        data_object_properties = []\n",
    "        for obj in data_objects.objects:\n",
    "            row = dict(obj.properties)\n",
    "            row[uuid_column] = str(obj.uuid)\n",
    "            data_object_properties.append(row)\n",
    "\n",
    "    documents_to_uuid.update(\n",
    "        self._prepare_document_to_uuid_map(\n",
    "            data=data_object_properties,\n",
    "            group_key=document_column,\n",
    "            get_value=lambda x: x[uuid_column],\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\\n",
    "changed_documents = set()\n",
    "unchanged_docs = set()\n",
    "new_documents = set()\n",
    "\n",
    "\n",
    "# segregate documents into changed, unchanged and non-existing documents.\n",
    "for doc_url, doc_set in input_documents_to_uuid.items():\n",
    "    if doc_url in existing_documents_to_uuid\n",
    "        if existing_documents_to_uuid[doc_url] != doc_set:\n",
    "            changed_documents.add(str(doc_url))\n",
    "        else:\n",
    "            unchanged_docs.add(str(doc_url))\n",
    "    else:\n",
    "        new_documents.add(str(doc_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    (\n",
    "        documents_to_uuid_map,\n",
    "        changed_documents,\n",
    "        unchanged_documents,\n",
    "        new_documents,\n",
    "    ) = self._get_segregated_documents(\n",
    "        data=data,\n",
    "        document_column=document_column,\n",
    "        uuid_column=uuid_column,\n",
    "        collection_name=collection_name,\n",
    "    )\n",
    "    if verbose:\n",
    "        self.log.info(\n",
    "            \"Found %s changed documents, %s unchanged documents and %s non-existing documents\",\n",
    "            len(changed_documents),\n",
    "            len(unchanged_documents),\n",
    "            len(new_documents),\n",
    "        )\n",
    "        for document in changed_documents:\n",
    "            self.log.info(\n",
    "                \"Changed document: %s has %s objects.\", document, len(documents_to_uuid_map[document])\n",
    "            )\n",
    "        self.log.info(\"Non-existing document: %s\", \", \".join(new_documents))\n",
    "\n",
    "    if existing == \"error\" and len(changed_documents):\n",
    "        raise ValueError(\n",
    "            f\"Documents {', '.join(changed_documents)} already exists. You can either skip or replace\"\n",
    "            f\" them by passing 'existing=skip' or 'existing=replace' respectively.\"\n",
    "        )\n",
    "    elif existing == \"skip\":\n",
    "        data = data[data[document_column].isin(new_documents)]\n",
    "        if verbose:\n",
    "            self.log.info(\n",
    "                \"Since existing=skip, ingesting only non-existing document's object %s\", data.shape[0]\n",
    "            )\n",
    "    elif existing == \"replace\":\n",
    "        total_objects_count = sum([len(documents_to_uuid_map[doc]) for doc in changed_documents])\n",
    "        if verbose:\n",
    "            self.log.info(\n",
    "                \"Since existing='replace', deleting %s objects belonging changed documents %s\",\n",
    "                total_objects_count,\n",
    "                changed_documents,\n",
    "            )\n",
    "        if list(changed_documents):\n",
    "            batch_delete_error = self._delete_all_documents_objects(\n",
    "                document_keys=list(changed_documents),\n",
    "                document_column=document_column,\n",
    "                collection_name=collection_name,\n",
    "                total_objects_count=total_objects_count,\n",
    "                batch_delete_error=batch_delete_error,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "        data = data[data[document_column].isin(new_documents.union(changed_documents))]\n",
    "        self.log.info(\"Batch inserting %s objects for non-existing and changed documents.\", data.shape[0])\n",
    "\n",
    "    if data.shape[0]:\n",
    "        self.batch_data(\n",
    "            collection_name=collection_name,\n",
    "            data=data,\n",
    "            vector_col=vector_column,\n",
    "            uuid_col=uuid_column,\n",
    "        )\n",
    "        if batch_delete_error:\n",
    "            if batch_delete_error:\n",
    "                self.log.info(\"Failed to delete %s objects.\", len(batch_delete_error))\n",
    "            # Rollback object that were not created properly\n",
    "            self._delete_objects(\n",
    "                [item[\"uuid\"] for item in batch_delete_error],\n",
    "                collection_name=collection_name,\n",
    "            )\n",
    "\n",
    "    if verbose:\n",
    "        collection = self.get_collection(collection_name)\n",
    "        self.log.info(\n",
    "            \"Total objects in collection %s : %s \",\n",
    "            collection_name,\n",
    "            collection.aggregate.over_all(total_count=True),\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
